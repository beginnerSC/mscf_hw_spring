{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{center} \n",
    "Chun-Yuan (Scott) Chiu \n",
    "\\end{center}\n",
    "\\begin{center} \n",
    "chunyuac@andrew.cmu.edu \n",
    "\\end{center}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (adaboost_starter_code.py, line 121)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/srv/conda/envs/notebook/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3343\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-63d65331c939>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from adaboost_starter_code import get_circle_data\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/jovyan/mscf_hw_spring/ML II/1/adaboost_starter_code.py\"\u001b[0;36m, line \u001b[0;32m121\u001b[0m\n\u001b[0;31m    alphas[b] = # TODO: Add the new tree's weight\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from adaboost_starter_code import get_circle_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1)],\n",
       " array([0.42634269, 0.59359126, 0.45201697, 0.59396359, 0.45558533,\n",
       "        0.69258522, 0.50648962, 0.70367107, 0.5123061 , 0.61447561]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#### These first 3 functions are free: you do not need to change them.\n",
    "\n",
    "def find_split(X, y, wgts=None):\n",
    "    \"\"\"Find the best single split on one of the two variables, while incorporating weights.\n",
    "\n",
    "    Inputs:\n",
    "    X: a numpy array with 2 columns\n",
    "    y: a numpy array with n elements\n",
    "    wgts: a numpy array of n observation weights.\n",
    "\n",
    "    Return two things:\n",
    "    fit: The object returned by DecisionTreeClassifier\n",
    "    tree_info: A dictionary with the following additional info. You don't need\n",
    "    this, but it might help debug:\n",
    "\n",
    "    variable_idx: Number of selected variable column\n",
    "    threshold: Where that variable was split\n",
    "    guess_less: which category is guessed if less than threshold\n",
    "    guess_greater: which category is guessed if greater than threshold\n",
    "    \"\"\"\n",
    "\n",
    "    n = X.shape[0]\n",
    "    dtc = DecisionTreeClassifier(max_depth=1)\n",
    "    fit = dtc.fit(X,y, sample_weight=wgts)\n",
    "    tree_info = {\n",
    "        'variable_idx' : fit.tree_.feature[0],\n",
    "        'threshold' : fit.tree_.threshold[0],\n",
    "        'guess_less' : np.argmax(fit.tree_.value[1])*2-1,\n",
    "        'guess_greater' : np.argmax(fit.tree_.value[2])*2-1\n",
    "    }\n",
    "\n",
    "    return fit, tree_info\n",
    "\n",
    "\n",
    "def draw_boosted_trees(trees, X, score=None):\n",
    "    \"\"\"Draw the data points and the decision boundaries used by the boosted trees.\n",
    "\n",
    "    If score is supplied, the points are also shaded by the score.\n",
    "\n",
    "    Inputs:\n",
    "    trees: the list of tree objects output by my_adaboost\n",
    "    X: a set of points output by get_circle_data (no y is needed)\n",
    "    score: n-vector of scores to color each point (optional)\n",
    "    Outputs: makes a plot.  You still need to call plt.show()\n",
    "    \"\"\"\n",
    "\n",
    "    if score is not None:\n",
    "        plt.scatter(X[:,0], X[:,1], c=score, cmap=plt.get_cmap('plasma'))\n",
    "    else:\n",
    "        plt.scatter(X[:,0], X[:,1])\n",
    "\n",
    "    line_col='grey'\n",
    "\n",
    "    for fit in trees:\n",
    "        variable_idx = fit.tree_.feature[0]\n",
    "        threshold = fit.tree_.threshold[0]\n",
    "\n",
    "        if variable_idx == 0:\n",
    "            plt.axvline(x=threshold, color=line_col)\n",
    "        else:\n",
    "            plt.axhline(y=threshold, color=line_col)\n",
    "\n",
    "\n",
    "def get_circle_data(n):\n",
    "    \"\"\"Generate data for our example\n",
    "\n",
    "    Inputs:\n",
    "    n: number of samples\n",
    "\n",
    "    Output: tuple with entries:\n",
    "    X: an nx2 matrix of data points.\n",
    "    y: class for the point (-1,1)\n",
    "    \"\"\"\n",
    "\n",
    "    gaussian_2d = scipy.stats.multivariate_normal(mean = np.zeros(2), cov = np.eye(2))\n",
    "    X = gaussian_2d.rvs(n)\n",
    "    Y = np.where(X[:,0]**2 + X[:,1]**2 < 1, 1, -1)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "#### YOU DON'T NEED TO CHANGE FUNCTIONS ABOVE THIS LINE, THEY ARE FREE ####\n",
    "\n",
    "#### You must finish the functions below this line ####\n",
    "\n",
    "def my_adaboost(X, y, B=10):\n",
    "    \"\"\"Implement the adaboost function in my_adaboost\n",
    "\n",
    "    Inputs:\n",
    "    X, y: Data produced by get_circle_data()\n",
    "    B: The number of trees to produce during boosting\n",
    "\n",
    "    Returns:\n",
    "    trees: a list of fitted tree objects\n",
    "    alphas: a numpy array of B alpha values for the trees\n",
    "    \"\"\"\n",
    "\n",
    "    n = X.shape[0]\n",
    "    # Equal observation weights\n",
    "    wgts = np.repeat(1/n, n)\n",
    "    trees = [] # To store your trees.  Use append()\n",
    "    alphas = np.zeros(B) # To store your alphas\n",
    "\n",
    "    for b in range(B):\n",
    "        # Find the next split, according to your current weights.\n",
    "        # Hint: the returned tree supports predict()\n",
    "        tree, info = find_split(X, y, wgts)\n",
    "        \n",
    "        misclf = (tree.predict(X) != y)\n",
    "        eb = (wgts*misclf).sum()/wgts.sum()\n",
    "        alpha = np.log((1-eb)/eb)\n",
    "        alphas[b] = alpha# TODO: Add the new tree's weight\n",
    "        trees.append(tree) # TODO: Store the new tree\n",
    "        wgts *= np.exp(alpha*misclf)# TODO: Update your observation weights\n",
    "\n",
    "    return trees, alphas\n",
    "\n",
    "X, y = get_circle_data(1000)\n",
    "\n",
    "my_adaboost(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-5.74993067, -5.69069289, -5.69069289, -5.74993067,  2.95492166,\n",
       "        -5.74993067, -5.74993067,  2.95492166, -5.74993067, -5.69069289]),\n",
       " array([-1., -1., -1., -1.,  1., -1., -1.,  1., -1., -1.]),\n",
       " array([0.2, 0.3, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_ada(trees, alphas, X, y=None):\n",
    "    \"\"\"Calculate predictions for boosted trees from my_adaboost.\n",
    "\n",
    "    If passed true y values, also computes *running* misclassification error\n",
    "    across all B.\n",
    "\n",
    "    Inputs:\n",
    "    trees: list of trees returned by my_adaboost\n",
    "    alphas: alpha vector returned by my_adaboost\n",
    "    X: a data set to make predictions on\n",
    "    y: the true labels for that data set, in {-1,1}.  If given, running prediction error is computed.\n",
    "\n",
    "    Outputs: a list containing:\n",
    "    score: the score for each point (before thresholding)\n",
    "    preds: the thresholded scores: our guesses at class.\n",
    "    test_err: a length B vector of misclassification error at each step (1 tree to B trees)\n",
    "    \"\"\"\n",
    "\n",
    "    n = X.shape[0]\n",
    "    B = len(trees)\n",
    "\n",
    "    # Initialize a score vector to zero.\n",
    "    # This will store the ongoing sum alpha_b fhat_b(x) for each point.\n",
    "    score = np.zeros(n)\n",
    "\n",
    "    # This will store the misclassification error at each step,\n",
    "    # so we can see how it changes with the number of trees\n",
    "    test_err = np.zeros(B)\n",
    "\n",
    "    for b in range(B):\n",
    "        # Update your score to include the contribution from tree b\n",
    "        score += alphas[b]*trees[b].predict(X) # TODO: Fill in the updated score\n",
    "        preds = np.sign(score) # TODO: Make predictions in {-1,1}\n",
    "\n",
    "        if y is not None:\n",
    "            # Record the misclassification error for the predictions\n",
    "            # based on thresholding the current score\n",
    "            test_err[b] = (y!=preds).mean() # TODO: Fill in the current misclassification error\n",
    "            \n",
    "    if y is None:\n",
    "        return score, preds\n",
    "\n",
    "    return score, preds, test_err\n",
    "\n",
    "X, y = get_circle_data(10)\n",
    "trees, alphas = my_adaboost(X, y)\n",
    "predict_ada(trees, alphas, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_partial_dependence_x1(trees, alphas, X):\n",
    "    \"\"\"Calculates the partial dependence plot over the first column of X.\n",
    "\n",
    "    Fixes the first column at each point in the grid and evaluates the\n",
    "    predictions at all of these fake points.  These predictions are used\n",
    "    to obtain the partial dependence over x1.\n",
    "\n",
    "    Inputs:\n",
    "    trees: list of trees returned by my_adaboost\n",
    "    alphas: list of alphas returned by my_adaboost\n",
    "    X: data set to use for computing partial dependence.\n",
    "    \"\"\"\n",
    "\n",
    "    m = 50 # Number of grid points to evaluate partial dependence\n",
    "    x = np.linspace(np.min(X[:,0]), np.max(X[:,0]), m)\n",
    "\n",
    "    # Somewhere to store the partial dependence values\n",
    "    pdep = np.zeros(m)\n",
    "\n",
    "    # We will fake a dataset and change its x1 values to match each evaluation points\n",
    "    fake = X.copy()\n",
    "    for i in range(m):\n",
    "        fake[:,0] = x[i]\n",
    "        # TODO: Fill in the actual calculation of the partial\n",
    "        # dependence at x[i] below\n",
    "        # HINT: Evaluate predict_ada on the fake data set and use\n",
    "        # resulting predictions\n",
    "        pdep[i] = # TODO\n",
    "\n",
    "    return x, pdep"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
