{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{center} \n",
    "Chun-Yuan (Scott) Chiu \n",
    "\\end{center}\n",
    "\\begin{center} \n",
    "chunyuac@andrew.cmu.edu \n",
    "\\end{center}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# {-}\n",
    "\n",
    "This is starter code for homework 6.  Most of the code won't run as is.  You need to complete the missing parts of the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (12,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part a {-}\n",
    "\n",
    "Fill in the missing parts of the classes below, marked with TODO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class represents your loss function, (1/2) sum((y-yhat)^2).\n",
    "# Note: we are including a 1/2, which we don't always do, so that your derivatives will be particularly nice.\n",
    "# The functions should operate on vectors y and yhat\n",
    "class Loss(object):\n",
    "    def __init__(self):\n",
    "        # These fields will store the last seen y and yhat\n",
    "        # The stored values will be used by backward() to compute the gradient of this function\n",
    "        # at the last evaluated point\n",
    "        self.last_y = 0\n",
    "        self.last_yhat = 0\n",
    "    \n",
    "    # This function returns the value of the loss\n",
    "    def forward(self, y, yhat):\n",
    "        # Store the new values for use in the gradient\n",
    "        self.last_y = y\n",
    "        self.last_yhat = yhat    \n",
    "        return 0.5*((self.last_y - self.last_yhat)**2).sum()\n",
    "    \n",
    "    # This function returns the gradient of the loss: y - yhat\n",
    "    # Important: This must be an (n,1) matrix.  Not (n,). \n",
    "    def backward(self):\n",
    "        # Grab the length of y for shape checking\n",
    "        n = len(self.last_y)\n",
    "        return_value = -(self.last_y - self.last_yhat)\n",
    "        \n",
    "        # Let's check the shape of the return value to keep you sane\n",
    "        assert return_value.shape == (n,1), \"Loss backward returns wrong shape\"\n",
    "        return return_value\n",
    "    \n",
    "# This class represents your linear function, beta -> X\\beta.  Should work for an n by p matrix X.\n",
    "class LinearFunction(object):\n",
    "    # Linear function is initialized with a dimension p, and an optional starting value beta\n",
    "    def __init__(self, p, beta=None):\n",
    "        self.p = p\n",
    "        if beta is None:\n",
    "            self.beta = np.zeros((p,1))\n",
    "        else:\n",
    "            #We make sure that beta starts with the right shape, even if you send in the wrong shape\n",
    "            self.beta = beta.reshape((p,1))\n",
    "        #Store the last X seen by this function, initialize to 0\n",
    "        self.last_X = 0\n",
    "    \n",
    "    # This function returns the value of the linear function at an X and the current beta: X*beta\n",
    "    def forward(self, X):\n",
    "        # Record the new X for use later in the gradient\n",
    "        n,p = X.shape\n",
    "        self.last_X = X\n",
    "        \n",
    "        # TODO: You need to compute the value of your linear function: X*beta (matrix multiplied)\n",
    "        # Make sure that your return value is n by 1\n",
    "        return_value = self.last_X @ self.beta\n",
    "        \n",
    "        assert return_value.shape == (n,1), \"Linear forward returns wrong shape\"\n",
    "        return return_value\n",
    "    \n",
    "    def backward(self):\n",
    "        # TODO: You need to compute the gradient of the linear function with respect to beta. (X^T)\n",
    "        # Make sure that your return value is p by n\n",
    "        n,p = self.last_X.shape\n",
    "        \n",
    "        return_value = self.last_X.T\n",
    "        \n",
    "        # Let's check the shape of the return value to keep you sane\n",
    "        assert return_value.shape == (p,n), \"Loss backward returns wrong shape\"\n",
    "        return return_value\n",
    "    \n",
    "    # This is just here to make your future updates ever so slightly easier.  This way your gradient updates \n",
    "    # will just be a function call, and the shape will be confirmed correct\n",
    "    def beta_shift(self, shift):\n",
    "        assert self.beta.shape == shift.shape\n",
    "        self.beta = self.beta + shift\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all there is to it!  We can link them together using the chain rule, and then solve linear regression by coordinate descent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate some trial data to experiment with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "p = 3\n",
    "beta = np.array([3,1,4]).reshape((-1,1))\n",
    "np.random.seed(1)\n",
    "X = scipy.stats.norm.rvs(size=n*p).reshape((n,p))\n",
    "y = np.dot(X,beta) + scipy.stats.norm.rvs(size=n, scale=0.1).reshape((n,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.00263927, 0.98650739, 3.99947187]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the coefficient is about what we think it should be.\n",
    "# Hopefully you will soon have an implementation that agrees with this.\n",
    "fit = LinearRegression().fit(X,y)\n",
    "fit.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check 1a {-}\n",
    "\n",
    "Execute this code to check your implementation.  This should execute without errors and without triggering the assertions about return value shapes.  Summaries are computed of the gradients to verify you're getting the right numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1087.1524810665376\n",
      "[0.0179277  0.08671813 0.11855369]\n",
      "-0.6126797649214445\n"
     ]
    }
   ],
   "source": [
    "# We can initialize the pieces of our solver. Right now, beta is initialized to a vector of zeros\n",
    "linear = LinearFunction(p)\n",
    "loss = Loss()\n",
    "\n",
    "# Computes the squared error loss for your current X, evaluated with beta = 0\n",
    "print(loss.forward(y, linear.forward(X))) # Should be approximately 1087.15\n",
    "\n",
    "# Compute the gradient of the linear piece and summarize by the mean for checking\n",
    "print(np.mean(linear.backward(), axis=1))\n",
    "# Should be [0.0179277 , 0.08671813, 0.11855369]\n",
    "\n",
    "#Compute the gradient of the loss with respect to yhat and summarize by mean\n",
    "print(np.mean(loss.backward())) # Should be approximately -0.613"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part b {-}\n",
    "\n",
    "Put it all together into what will become one iteration of gradient descent.  The end goal here is to produce a gradient of the loss with respect to beta, which should have shape p x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-283.8105542 ]\n",
      " [ -39.20095874]\n",
      " [-320.63943886]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Fill in each of the missing lines\n",
    "# Compute the loss for linear function evaluated at current X\n",
    "## TODO ##\n",
    "\n",
    "# Compute the gradient of the loss with respect to beta.\n",
    "# This should be (gradient from linear) * (gradient from loss)\n",
    "grad = linear.backward() @ loss.backward()\n",
    "\n",
    "# Print out your final gradient of the loss with respect to beta for the first step from initialization above.\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part c {-}\n",
    "\n",
    "Now you will actually write gradient descent and solve the problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of steps of gradient descent\n",
    "reps = 100\n",
    "# Learning rate (in practice, often decreases with iteration)\n",
    "eta = 0.01\n",
    "\n",
    "# Initialize the pieces of our solver. beta is initialized to a vector of zeros\n",
    "linear = LinearFunction(p)\n",
    "loss = Loss()\n",
    "\n",
    "# Let's keep track of the loss at each step, so you can visualize your optimization later\n",
    "losses = np.zeros(reps)\n",
    "\n",
    "grads = []\n",
    "\n",
    "for i in range(reps):\n",
    "    # Compute and store your current loss\n",
    "    losses[i] = loss.forward(y, linear.forward(X))\n",
    "    \n",
    "    # Compute the gradient of loss with respect to beta\n",
    "    grad = linear.backward() @ loss.backward()\n",
    "    grads.append(grad)\n",
    "    \n",
    "    # Adjust beta (using beta_shift) by -grad*eta\n",
    "    linear.beta_shift(-grad*eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part d {-}\n",
    "\n",
    "The training result $\\beta$, as printed out here, is very close to the benchmark $(3, 1, 4)^T$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.00261269],\n",
       "       [0.98641329],\n",
       "       [3.99934409]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss decreases rapidly towards zero and converges within the first few iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pandas import Series\n",
    "\n",
    "Series(losses).plot(style='.-')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part e {-}\n",
    "\n",
    "Stochastic gradient descent.  Here you will change your solver to do stochastic gradient descent.  Not much has to change, just your loop from part c.  The Linear and Loss classes stay the same, we just don't show them all the data at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of steps of gradient descent.  We need more for the stochastic version\n",
    "reps = 1000\n",
    "# Learning rate (in practice, often decreases with iteration)\n",
    "eta = 0.01\n",
    "\n",
    "# Initialize the pieces of our solver. beta is initialized to a vector of zeros\n",
    "linear = LinearFunction(p)\n",
    "loss = Loss()        \n",
    "\n",
    "# Let's keep track of the loss at each step, so you can visualize your optimization later.  \n",
    "# We will track two losses: The loss on your single sampled point, and the loss on your whole sample.\n",
    "losses = np.zeros(reps)\n",
    "losses_entire = np.zeros(reps)\n",
    "\n",
    "for i in range(reps):\n",
    "    # Pick a single observation to show the optimizer this time\n",
    "    k = np.random.randint(y.shape[0])\n",
    "    \n",
    "    # Compute and store your current loss, only for the kth row of X and kth y element!\n",
    "    # Note: when subsetting X, use X[np.newaxis, k,:].  This keeps the resulting row\n",
    "    # two dimensional, rather than dropping to a 1d vector.\n",
    "    losses[i] = ## TODO: Fill me in ##\n",
    "    \n",
    "    # Compute the gradient of loss with respect to beta\n",
    "    grad = ## TODO: Fill me in ##\n",
    "    \n",
    "    # Compute losses on the whole sample (you wouldn't usually actually do this, too expensive)\n",
    "    losses_entire[i] = loss.forward(y, linear.forward(X))\n",
    "    \n",
    "    # Adjust beta (using beta_shift) by -grad*eta\n",
    "    ## TODO: Fill me in ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same data as before\n",
    "n = 100\n",
    "p = 3\n",
    "beta = np.array([3,1,4]).reshape((-1,1))\n",
    "np.random.seed(1)\n",
    "X = scipy.stats.norm.rvs(size=n*p).reshape((n,p))\n",
    "y = np.dot(X,beta) + scipy.stats.norm.rvs(size=n, scale=0.1).reshape((n,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your data X, y to tensors, using torch.Tensor\n",
    "X = torch.Tensor(X)\n",
    "y = torch.Tensor(y)\n",
    "\n",
    "# Convert those tensors to variables, so that torch will track gradients.  Use torch.autograd.Variable\n",
    "X = torch.autograd.Variable(X)\n",
    "y = torch.autograd.Variable(y)\n",
    "    \n",
    "# Make a linear function using torch.nn.Linear\n",
    "# Set the bias to False turns off intercept. Not usally desirable, but want to match our problem 1 code.\n",
    "fc = torch.nn.Linear(p, 1, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 100\n",
    "losses = np.zeros(reps)\n",
    "for i in range(reps):\n",
    "    \n",
    "    # Reset gradients\n",
    "    fc.zero_grad()\n",
    "\n",
    "    # Forward pass: Compute the mse_loss between fc(X) and y, using F.mse_loss(guess, truth)\n",
    "    # Store the output in losses[i]\n",
    "    output = F.mse_loss(fc(X), y) ## TODO ##\n",
    "    losses[i] = output\n",
    "    \n",
    "    # Compute all the gradients\n",
    "    output.backward()\n",
    "\n",
    "    # Apply gradients.  In practice, this can be carried out by built in functions\n",
    "    # as well, like torch.optim.SGD\n",
    "    for param in fc.parameters():\n",
    "        param.data.add_(-0.1 * param.grad.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
